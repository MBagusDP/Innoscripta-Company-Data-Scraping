{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT PACAKGES\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import httplib2\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from googleapiclient import discovery\n",
    "from oauth2client import client\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, ElementNotInteractableException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS\n",
    "print('Define functions...')\n",
    "\n",
    "def get_config(config_set):\n",
    "    cred_path = 'id_gsheet.yml'\n",
    "    config = yaml.safe_load(open(cred_path))\n",
    "    config_set = config['id'][config_set]\n",
    "    return config_set\n",
    "\n",
    "\n",
    "\n",
    "def get_credentials(cred_set, json_configs=''):\n",
    "    credentials = None\n",
    "    if json_configs != '':\n",
    "        credentials = get_credentials_from_json(json_configs)\n",
    "    else:\n",
    "        client_secret = cred_set['client_secret']\n",
    "        grant_type = cred_set['grant_type']\n",
    "        refresh_token = cred_set['refresh_token']\n",
    "        client_id = cred_set['client_id']\n",
    "        url = cred_set['url']\n",
    "        data = client_secret + '&' + grant_type + '&' + refresh_token + '&' + client_id\n",
    "        response = json.loads(subprocess.check_output(['curl', '--data', data, url]))\n",
    "        access_token = response[\"access_token\"]\n",
    "        credentials = client.AccessTokenCredentials(access_token, 'my-user-agent/1.0')\n",
    "        http = httplib2.Http()\n",
    "        http = credentials.authorize(http)\n",
    "    return credentials\n",
    "\n",
    "        \n",
    "\n",
    "def wait_for_visibility(tag_name, elem_name, try_count):\n",
    "        switcher = {\n",
    "            \"ID\": By.ID,\n",
    "            \"NAME\": By.NAME,\n",
    "            \"CLASS_NAME\": By.CLASS_NAME,\n",
    "            \"XPATH\": By.XPATH,\n",
    "            \"CSS_SELECTOR\": By.CSS_SELECTOR,\n",
    "            \"TAG_NAME\": By.TAG_NAME\n",
    "        }\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.visibility_of_element_located((switcher.get(tag_name), elem_name)))\n",
    "        except TimeoutException:\n",
    "            if try_count > 9:\n",
    "                print(\"\\nPage reloaded more than 10 times!\")\n",
    "                print(\"\\nProgram terminated!\")\n",
    "                sys.exit()\n",
    "            print(\"Error: Element '\" + elem_name + \"' not found!\")\n",
    "            return \"Timeout\"\n",
    "\n",
    "\n",
    "\n",
    "def wait_for_invisibility(tag_name, elem_name, try_count):\n",
    "    switcher = {\n",
    "        \"ID\": By.ID,\n",
    "        \"NAME\": By.NAME,\n",
    "        \"CLASS_NAME\": By.CLASS_NAME,\n",
    "        \"XPATH\": By.XPATH,\n",
    "        \"CSS_SELECTOR\": By.CSS_SELECTOR,\n",
    "        \"TAG_NAME\": By.TAG_NAME\n",
    "    }\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((switcher.get(tag_name), elem_name)))\n",
    "    except TimeoutException:\n",
    "        if try_count > 9:\n",
    "            print(\"\\nPage reloaded more than 10 times!\")\n",
    "            print(\"\\nProgram terminated!\")\n",
    "            sys.exit()\n",
    "        print(\"Error: Element '\" + elem_name + \"' still present after 10 seconds!\")\n",
    "        return \"Timeout\"\n",
    "\n",
    "\n",
    "\n",
    "def click_elem(tag_name, elem_name, try_count):\n",
    "    switcher = {\n",
    "        \"ID\": By.ID,\n",
    "        \"NAME\": By.NAME,\n",
    "        \"CLASS_NAME\": By.CLASS_NAME,\n",
    "        \"XPATH\": By.XPATH,\n",
    "        \"CSS_SELECTOR\": By.CSS_SELECTOR,\n",
    "        \"TAG_NAME\": By.TAG_NAME\n",
    "    }\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).\\\n",
    "            until(EC.element_to_be_clickable((switcher.get(tag_name), elem_name)))\n",
    "        try:\n",
    "            element.click()\n",
    "        except StaleElementReferenceException:\n",
    "            if try_count > 9:\n",
    "                print(\"\\nPage reloaded more than 10 times!\")\n",
    "                print(\"\\nProgram terminated!\")\n",
    "                sys.exit()\n",
    "            print(\"Error: Element '\" + elem_name + \"' not attached to the page document!\")\n",
    "            return \"Timeout\"\n",
    "        except ElementNotInteractableException:\n",
    "            if try_count > 9:\n",
    "                print(\"\\nPage reloaded more than 10 times!\")\n",
    "                print(\"\\nProgram terminated!\")\n",
    "                sys.exit()\n",
    "            print(\"Error: Element '\" + elem_name + \"' not interactable!\")\n",
    "            return \"Timeout\"\n",
    "    except TimeoutException:\n",
    "        if try_count > 9:\n",
    "            print(\"\\nPage reloaded more than 10 times!\")\n",
    "            print(\"\\nProgram terminated!\")\n",
    "            sys.exit()\n",
    "        print(\"Error: Element '\" + elem_name + \"' not clickable!\")\n",
    "        return \"Timeout\"\n",
    "\n",
    "\n",
    "\n",
    "def injection(spreadsheetId, df, rangeName):\n",
    "    #generate dataframe\n",
    "    df.fillna('')\n",
    "    df = df.astype(str)\n",
    "    df.replace(['NaN'],'',inplace= True)\n",
    "    df_columns = df.columns.values.tolist()\n",
    "    df_content = df.values.tolist()\n",
    "    df_content.insert(0, df_columns)\n",
    "\n",
    "    cred_set =  get_config('spreadsheet')\n",
    "    credentials = get_credentials(cred_set)\n",
    "    service = discovery.build('sheets', 'v4', credentials=credentials)\n",
    "    list = df_content\n",
    "    resource = {\n",
    "      \"majorDimension\": \"ROWS\",\n",
    "      \"values\": list\n",
    "    }\n",
    "    #delete all the input\n",
    "    service.spreadsheets().values().clear(\n",
    "        spreadsheetId=spreadsheetId,\n",
    "        range=rangeName\n",
    "    ).execute()\n",
    "    #add all input\n",
    "    service.spreadsheets().values().append(\n",
    "      spreadsheetId=spreadsheetId,\n",
    "      range=rangeName,\n",
    "      body=resource,\n",
    "      valueInputOption=\"USER_ENTERED\"\n",
    "    ).execute()\n",
    "\n",
    "\n",
    "\n",
    "def read_gsheet(file_id_properti, rangeName):\n",
    "    cred_set =  get_config('spreadsheet')\n",
    "    credentials = get_credentials(cred_set)\n",
    "    service = discovery.build('sheets', 'v4', credentials=credentials)\n",
    "    content = service.spreadsheets().values().get(spreadsheetId=file_id_properti, range=rangeName).execute()\n",
    "    #data transform from gsheet to dataframe\n",
    "    headers = list(content['values'])[0]\n",
    "    df = pd.DataFrame(data=np.zeros((0,len(headers))), columns=headers)\n",
    "    result_df = pd.DataFrame(content['values'],columns = headers)\n",
    "    df  = pd.concat([df ,result_df],axis=0)\n",
    "    df = df.iloc[1:].reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def login(try_count):\n",
    "    print('Go to '+website+'...')\n",
    "    driver.get(website)\n",
    "    current_page_url = driver.current_url\n",
    "    print('Landed on '+current_page_url+'!')\n",
    "    time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC \n",
    "\n",
    "    if 'public/login' in current_page_url:\n",
    "        while True:\n",
    "            print('Initiating logging in to the website...')\n",
    "            if wait_for_visibility('XPATH', \"//input[@placeholder='Email address']\", try_count) != 'Timeout':\n",
    "\n",
    "                driver.find_element(By.XPATH, \"//input[@placeholder='Email address']\").send_keys(id_innoscripta)\n",
    "                print(driver.find_element(By.XPATH, \"//input[@placeholder='Email address']\").get_attribute('value'))\n",
    "                time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "\n",
    "                driver.find_element(By.XPATH, \"//input[@placeholder='Password']\").send_keys(pass_innoscripta)\n",
    "                print(driver.find_element(By.XPATH, \"//input[@placeholder='Password']\").get_attribute('value'))\n",
    "                time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "\n",
    "                print(driver.find_element(By.XPATH, \"//button[@type='submit']\").get_attribute('type'))\n",
    "                click_elem('XPATH', '//button[@type=\"submit\"]', try_count)\n",
    "                try_count = 0\n",
    "                time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "\n",
    "                print('Login successful!')\n",
    "                current_page_url = driver.current_url\n",
    "                print('Landed on '+current_page_url+'!')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print('Element not found!')\n",
    "                print('Refreshing page...')\n",
    "                try_count += 1\n",
    "                driver.get(current_page_url)\n",
    "                time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "    else:\n",
    "        print('Logged in status detected, go to https://platform.globaldatabase.com/app-aggregator/prospect/companies')\n",
    "        driver.get('https://platform.globaldatabase.com/app-aggregator/prospect/companies')\n",
    "        current_page_url = driver.current_url\n",
    "        time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "\n",
    "    return try_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP SELENIUM PYTHON\n",
    "print('Setting up Selenium...')\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "# chrome_options.add_experimental_option( \"prefs\",{'profile.managed_default_content_settings.javascript': 1})\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "print('Selenium is ready to be used!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGIN TO GLOBAL DATABASE AND SET FILTER CANADA\n",
    "id_innoscripta = {put your user id here}\n",
    "pass_innoscripta = {put your password here}\n",
    "website = 'https://platform.globaldatabase.com/public/login'\n",
    "current_page_url = website\n",
    "random_sleep = [2,2.5,3,3.5,4,4.5,5]\n",
    "try_count = 0\n",
    "\n",
    "try_count = login(try_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN IF NEW GSHEET IS CREATED TO STORE THE DATA\n",
    "# overview_data = pd.DataFrame([], columns=['company_name', 'employees', 'age', 'country', 'address', 'phones', 'faxes', 'emails', 'website', 'incorporated',\n",
    "#                                         'sic_classification', 'company_type', 'industries', 'shareholders', 'accounts', 'banking_with', 'registration_number', \n",
    "#                                         'vat_number', 'description', 'monthly_web_traffic', 'number_of_tech_used', 'domain_registered'])\n",
    "# employees_data = pd.DataFrame([], columns=['cxo', 'director', 'manager', 'non-manager', 'partner', 'senior', 'vp'])\n",
    "# digital_insight_data = pd.DataFrame([], columns=['monthly_web_traffic', 'website', 'last_6_month_visitor', 'organic_search', 'direct_traffic', 'referral_traffic', 'page_views', 'average_session', 'bounce_rate', 'top_traffic_by_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET RECORDS FROM GSHEET\n",
    "print('Get existing records from gsheet...')\n",
    "overview_gsheet = read_gsheet(file_id, 'Company Overview!A1:AZ')\n",
    "employees_gsheet = read_gsheet(file_id, 'Company Employees!A1:AZ')\n",
    "digital_insight_gsheet = read_gsheet(file_id, 'Company Digital Insights!A1:AZ')\n",
    "\n",
    "#SCRAPE THE DETAILS OF EACH COMPANIES\n",
    "file_id = '1cgSJetgMDgn0nmo0cf2bgfn84KfS4rU6udMlLheEb-4'\n",
    "\n",
    "try:\n",
    "    companies_gsheet\n",
    "except:\n",
    "    companies_gsheet = read_gsheet(file_id, 'Company List!A1:AZ').values.tolist()\n",
    "\n",
    "try:\n",
    "    overview_data,\n",
    "    employees_data,\n",
    "    digital_insight_data\n",
    "except:\n",
    "    overview_data = overview_gsheet.copy()\n",
    "    employees_data = employees_gsheet.copy()\n",
    "    digital_insight_data = digital_insight_gsheet.copy()\n",
    "\n",
    "print('Get Companies details...')\n",
    "itter = 0\n",
    "try:\n",
    "    for company in companies_gsheet:\n",
    "        try:\n",
    "            if overview_data.company_name[itter] != '' and employees_data.cxo[itter] != '' and digital_insight_data.monthly_web_traffic[itter] != '':\n",
    "                itter += 1\n",
    "                continue\n",
    "        except:\n",
    "            'continue'\n",
    "                                            \n",
    "        print('Itteration: '+str(itter)+'...')\n",
    "        print('Get details of: '+company[0]+'...')\n",
    "        overview_data.loc[overview_data.shape[0]] = ['','','','','','','','','','','','','','','','','','','','','','']\n",
    "        employees_data.loc[employees_data.shape[0]] = ['','','','','','','']\n",
    "        digital_insight_data.loc[digital_insight_data.shape[0]] = ['','','','','','','','','','']\n",
    "\n",
    "        # sleep_random = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "        driver.get(company[1])\n",
    "        # time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "        current_page_url = driver.current_url\n",
    "        print('Landed on '+current_page_url+'!')\n",
    "        try_count = 0\n",
    "\n",
    "        if driver.current_url == 'https://platform.globaldatabase.com/app-aggregator/account/products/metrics?error_guard=limits.daily&detail_error=aggregator':\n",
    "            #WHEN DAILY LIMIT REACHED, SAVE THE RESULT TO GHSEET\n",
    "            injection(file_id, overview_data, 'Company Overview!A1:AZ')\n",
    "            injection(file_id, employees_data, 'Company Employees!A1:AZ')\n",
    "            injection(file_id, digital_insight_data, 'Company Digital Insights!A1:AZ')\n",
    "            print('Daily limit has been reached!')\n",
    "            break\n",
    "\n",
    "        #################################################   OVERVIEW   #################################################\n",
    "        while True:\n",
    "            if wait_for_visibility('XPATH', \"//div[@class='company-details-block']\", try_count) != 'Timeout':\n",
    "                detail_block = []\n",
    "                for elem in driver.find_elements(By.XPATH, \"//div[@class='company-details-block']\"):\n",
    "                    detail_block.append(elem.text.split('\\n'))\n",
    "\n",
    "                try: overview_data.loc[overview_data.index[itter], 'company_name'] = driver.find_element(By.XPATH, \"//span[@class='title-company-name']\").text\n",
    "                except: overview_data.loc[overview_data.index[itter], 'company_name'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                try: overview_data.loc[overview_data.index[itter], 'employees'] = detail_block[0][1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'employees'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                try: overview_data.loc[overview_data.index[itter], 'age'] = detail_block[0][3]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'age'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                try: overview_data.loc[overview_data.index[itter], 'monthly_web_traffic'] = detail_block[1][1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'monthly_web_traffic'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                try: overview_data.loc[overview_data.index[itter], 'number_of_tech_used'] = detail_block[1][3]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'number_of_tech_used'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                try: overview_data.loc[overview_data.index[itter], 'domain_registered'] = detail_block[1][5]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'domain_registered'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "                profile = []\n",
    "                contact = []\n",
    "                for i in np.arange(0, len(driver.find_elements(By.XPATH, \"//div[@class='contacts-list-wrapper']\"))):\n",
    "                    profile = driver.find_elements(By.XPATH, \"//div[@class='contacts-list-wrapper']\")[0].text.split('\\n')\n",
    "                    # time.sleep(random.choice(sleep_random))\n",
    "                    contact = driver.find_elements(By.XPATH, \"//div[@class='contacts-list-wrapper']\")[1].text.split('\\n')\n",
    "                    # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "                #CONTACT\n",
    "                text = ''\n",
    "                for i in np.arange(contact.index('Country of origin:')+1, contact.index('Faxes:')):\n",
    "                    text += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'country'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'country'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(contact.index('Registered address:')+1, contact.index('Emails:')):\n",
    "                    text += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'address'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'address'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                phones_str = ''\n",
    "                for i in np.arange(contact.index('Phones:')+1, contact.index('Country of origin:')):\n",
    "                    phones_str += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'phones'] = phones_str[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'phones'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                faxes_str = ''\n",
    "                for i in np.arange(contact.index('Faxes:')+1, contact.index('Registered address:')):\n",
    "                    faxes_str += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'faxes'] = faxes_str[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'faxes'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(contact.index('Emails:')+1, contact.index('Websites:')):\n",
    "                    text += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'emails'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'emails'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(contact.index('Websites:')+1, contact.index('Social:')):\n",
    "                    text += contact[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'website'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'website'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "                #PROFILE\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('Incorporated:')+1, profile.index('SIC classification:')):\n",
    "                    text += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'incorporated'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'incorporated'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('SIC classification:')+1, profile.index('Company type:')):\n",
    "                    text += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'sic_classification'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'sic_classification'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('Company type:')+1, profile.index('Industries:')):\n",
    "                    text += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'company_type'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'company_type'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                industries_str = ''\n",
    "                for i in np.arange(profile.index('Industries:')+1, profile.index('Shareholder:')):\n",
    "                    industries_str += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'industries'] = industries_str[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'industries'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                shareholders_str = ''\n",
    "                for i in np.arange(profile.index('Shareholder:')+1, profile.index('Accounts:')):\n",
    "                    shareholders_str += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'shareholders'] = shareholders_str[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'shareholders'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                banking_with_str = ''\n",
    "                for i in np.arange(profile.index('Banking with:')+1, profile.index('Registration Number:')):\n",
    "                    banking_with_str += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'banking_with'] = banking_with_str[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'banking_with'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('Registration Number:')+1, profile.index('VAT Number:')):\n",
    "                    text += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'registration_number'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'registration_number'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('VAT Number:')+1, profile.index('Description:')):\n",
    "                    text += profile[i]+'|'\n",
    "                try: overview_data.loc[overview_data.index[itter], 'vat_number'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'vat_number'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "                text = ''\n",
    "                for i in np.arange(profile.index('Description:')+1, len(profile)-1):\n",
    "                    if profile[i] != '':\n",
    "                        text += profile[i]+' '\n",
    "                try: overview_data.loc[overview_data.index[itter], 'description'] = text[:-1]\n",
    "                except: overview_data.loc[overview_data.index[itter], 'description'] = np.nan\n",
    "                # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                if driver.current_url == 'https://platform.globaldatabase.com/public/login':\n",
    "                    print('HERE!')\n",
    "                    login(try_count)\n",
    "                    driver.get(company[1])\n",
    "                else:\n",
    "                    driver.get('https://platform.globaldatabase.com/app-aggregator/prospect/companies')\n",
    "                    time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "                    driver.get(company[1])\n",
    "                    try_count += 1\n",
    "                    if try_count > 2:\n",
    "                        login(try_count)\n",
    "                        try_count = 0\n",
    "                        # sys.exit('Error: try_count more than 2! ')\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        #################################################   EMPLOYEES   #################################################\n",
    "        tab_employees = driver.find_element(By.XPATH, \"//a[contains(text(), 'Employees')]\").get_attribute('href')\n",
    "        driver.get(tab_employees)\n",
    "        # time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "        current_page_url = driver.current_url\n",
    "        print('Landed on '+current_page_url+'!')\n",
    "        try_count = 0\n",
    "\n",
    "        if driver.current_url == 'https://platform.globaldatabase.com/app-aggregator/account/products/metrics?error_guard=limits.daily&detail_error=aggregator':\n",
    "            #WHEN DAILY LIMIT REACHED, SAVE THE RESULT TO GHSEET\n",
    "            injection(file_id, overview_data, 'Company Overview!A1:AZ')\n",
    "            injection(file_id, employees_data, 'Company Employees!A1:AZ')\n",
    "            injection(file_id, digital_insight_data, 'Company Digital Insights!A1:AZ')\n",
    "            print('Daily limit has been reached!')\n",
    "            break\n",
    "\n",
    "        while True:\n",
    "            if wait_for_visibility('CLASS_NAME', \"employees-counter-wrapper\", try_count) != 'Timeout':\n",
    "                    break\n",
    "            else:\n",
    "                    if driver.current_url == 'https://platform.globaldatabase.com/public/login':\n",
    "                        login(try_count)\n",
    "                        driver.get(tab_employees)\n",
    "                    else:\n",
    "                        driver.get('https://platform.globaldatabase.com/app-aggregator/prospect/companies')\n",
    "                        time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "                        driver.get(tab_employees)\n",
    "                        try_count += 1\n",
    "                        if try_count > 2:\n",
    "                            login(try_count)\n",
    "                            try_count = 0\n",
    "                            # sys.exit('Error: try_count more than 2! ')\n",
    "\n",
    "        wait_for_visibility('CLASS_NAME', 'employees-counter-wrapper', try_count)\n",
    "        employees_numbers = driver.find_element(By.CLASS_NAME, 'employees-counter-wrapper').text.split('\\n')\n",
    "        # time.sleep(random.choice(sleep_random))\n",
    "        try: employees_data.loc[employees_data.index[itter], 'cxo'] = employees_numbers[0]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'cxo'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'director'] = employees_numbers[2]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'director'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'manager'] = employees_numbers[4]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'manager'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'non-manager'] = employees_numbers[6]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'non-manager'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'partner'] = employees_numbers[8]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'partner'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'senior'] = employees_numbers[10]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'senior'] = np.nan\n",
    "        try: employees_data.loc[employees_data.index[itter], 'vp'] = employees_numbers[12]\n",
    "        except: employees_data.loc[employees_data.index[itter], 'vp'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "        #################################################   DIGITAL INSIGHTS   #################################################\n",
    "        wait_for_visibility('XPATH', \"//a[contains(text(), 'Digital Insights')]\", try_count)\n",
    "        # time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "\n",
    "        tab_digital_insight = driver.find_element(By.XPATH, \"//a[contains(text(), 'Digital Insights')]\").get_attribute('href')\n",
    "        driver.get(tab_digital_insight)\n",
    "        # time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "        current_page_url = driver.current_url\n",
    "        print('Landed on '+current_page_url+'!')\n",
    "        try_count = 0\n",
    "\n",
    "        if driver.current_url == 'https://platform.globaldatabase.com/app-aggregator/account/products/metrics?error_guard=limits.daily&detail_error=aggregator':\n",
    "            #WHEN DAILY LIMIT REACHED, SAVE THE RESULT TO GHSEET\n",
    "            injection(file_id, overview_data, 'Company Overview!A1:AZ')\n",
    "            injection(file_id, employees_data, 'Company Employees!A1:AZ')\n",
    "            injection(file_id, digital_insight_data, 'Company Digital Insights!A1:AZ')\n",
    "            print('Daily limit has been reached!')\n",
    "            break\n",
    "\n",
    "        try: \n",
    "            while True:\n",
    "                if wait_for_visibility('XPATH', \"//div[@class='company-details-block']\", try_count) != 'Timeout':\n",
    "                    break\n",
    "                else:\n",
    "                    if driver.current_url == 'https://platform.globaldatabase.com/public/login':\n",
    "                        login(try_count)\n",
    "                        driver.get(tab_digital_insight)\n",
    "                    else:\n",
    "                        driver.get('https://platform.globaldatabase.com/app-aggregator/prospect/companies')\n",
    "                        time.sleep(random.choice(random_sleep)) # RANDOMLY DELAYING ACTIVITY TO MAKE IT LESS ROBOTIC\n",
    "                        driver.get(tab_digital_insight)\n",
    "                        try_count += 1\n",
    "                        if try_count > 1:\n",
    "                            try_count = 0\n",
    "                            sys.exit('Error: try_count more than 2! ')\n",
    "\n",
    "            detail_block = []\n",
    "            for elem in driver.find_elements(By.XPATH, \"//div[@class='company-details-block']\"):\n",
    "                detail_block.append(elem.text.split('\\n'))\n",
    "            detail_block = list(np.concatenate(detail_block).flat)\n",
    "            # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'monthly_web_traffic'] = detail_block[1]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'monthly_web_traffic'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'website'] = detail_block[3]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'website'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'page_views'] = detail_block[5]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'page_views'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'average_session'] = detail_block[7]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'average_session'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'bounce_rate'] = detail_block[9]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'bounce_rate'] = np.nan\n",
    "\n",
    "            detail_block = []\n",
    "            for elem in driver.find_elements(By.XPATH, \"//div[@class='company-advises-block digital-insights-traffic']\"):\n",
    "                detail_block.append(elem.text.split('\\n'))\n",
    "            detail_block = list(np.concatenate(detail_block).flat)\n",
    "            # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'last_6_month_visitor'] = detail_block[2]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'last_6_month_visitor'] = np.nan\n",
    "            \n",
    "            detail_block = []\n",
    "            for elem in driver.find_elements(By.XPATH, \"//div[@class='company-advises-block']\"):\n",
    "                detail_block.append(elem.text.split('\\n'))\n",
    "            detail_block = list(np.concatenate(detail_block).flat)\n",
    "            # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'organic_search'] = detail_block[1]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'organic_search'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'direct_traffic'] = detail_block[4]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'direct_traffic'] = np.nan\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'referral_traffic'] = detail_block[7]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'referral_traffic'] = np.nan\n",
    "\n",
    "            detail_block = []\n",
    "            for elem in driver.find_elements(By.XPATH, \"//div[@class='ant-spin-container']\"):\n",
    "                detail_block.append(elem.text.split('\\n'))\n",
    "            detail_block = list(np.concatenate(detail_block).flat)\n",
    "            # time.sleep(random.choice(sleep_random))\n",
    "\n",
    "            text = ''\n",
    "            for i in np.arange(detail_block.index('Top Traffic by Country')+3, len(detail_block)-1):\n",
    "                text += detail_block[i]+'|'\n",
    "\n",
    "            try: digital_insight_data.loc[digital_insight_data.index[itter], 'top_traffic_by_country'] = text[:-1]\n",
    "            except: digital_insight_data.loc[digital_insight_data.index[itter], 'top_traffic_by_country'] = np.nan\n",
    "        \n",
    "        except:\n",
    "            if wait_for_visibility('XPATH', '//span[contains(text(), \"Unfortunately, no data was provided\")]', try_count) != 'Timeout':\n",
    "                print('Error: '+driver.find_element(By.XPATH, '//span[contains(text(), \"Unfortunately, no data was provided\")]').text)\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'monthly_web_traffic'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'website'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'page_views'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'average_session'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'bounce_rate'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'organic_search'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'direct_traffic'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'referral_traffic'] = np.nan\n",
    "                digital_insight_data.loc[digital_insight_data.index[itter], 'top_traffic_by_country'] = np.nan\n",
    "\n",
    "        itter += 1`\n",
    "except:\n",
    "    #IN CASE THE SCRAPER GET AN ERROR IN DURING EXECUTION, SAVE THE RESULT TO GSHEET\n",
    "    print('Error found during program execution, saving scrapped data to gsheet...')\n",
    "    injection(file_id, overview_data, 'Company Overview!A1:AZ')\n",
    "    injection(file_id, employees_data, 'Company Employees!A1:AZ')\n",
    "    injection(file_id, digital_insight_data, 'Company Digital Insights!A1:AZ')\n",
    "\n",
    "print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80f983b4dfedfae97556140c5073720c662441e2fdf0537233fc1759e0389af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
